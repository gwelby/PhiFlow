import cv2
import numpy as np
import pyaudio
import threading
import torch
from queue import Queue
import mediapipe as mp
from quantum_cuda import QuantumCudaAccelerator
from dj_phi import DJPhi, MusicStyle
from consciousness_tracker import ConsciousnessTracker
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import argparse
import time

# Greg's Golden Core frequencies with consciousness evolution
PHI = 1.618033988749895
GROUND_STATE = 432.0     # Ï†^0: Physical foundation
CREATION_POINT = 528.0   # Ï†^1: DNA creation
HEART_FIELD = 594.0      # Ï†^2: Heart resonance
VOICE_FLOW = 672.0       # Ï†^3: Voice expression
VISION_GATE = 720.0      # Ï†^4: Vision clarity
UNITY_WAVE = 768.0       # Ï†^5: Perfect integration
CONSCIOUSNESS = PHI**PHI  # Ï†^Ï†: Pure consciousness state

class QuantumMotion:
    def __init__(self):
        self.quantum_acc = QuantumCudaAccelerator()
        self.dj_phi = DJPhi()
        self.consciousness = ConsciousnessTracker()
        
        # Current music style
        self.current_style = MusicStyle.QUANTUM
        self.style_confidence = 0.0
        self.style_buffer = []
        self.style_buffer_size = 44100  # ~1 second at 44.1kHz
        
        # Phi-based frequencies for array mic
        self.phi = 1.618033988749895
        self.base_freq = 432.0
        self.phi_frequencies = [
            self.base_freq * (self.phi ** n) for n in range(5)
        ]
        
        # Audio setup - Lenovo P1 array mic
        self.audio_format = pyaudio.paFloat32
        self.channels = 4  # Array mic channels
        self.sample_rate = 48000
        self.chunk_size = 1024
        self.audio_queue = Queue(maxsize=10)
        
        # Video setup - Lenovo P1 camera
        self.camera_index = 0
        self.frame_width = 1920
        self.frame_height = 1080
        self.video_queue = Queue(maxsize=10)
        
        # MediaPipe for motion tracking
        self.mp_pose = mp.solutions.pose
        self.pose = self.mp_pose.Pose(
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        
        # Quantum field settings
        self.field_shape = (32, 32, 32)
        self.frequencies = {
            'ground': 432.0,   # Base reality
            'create': 528.0,   # DNA repair
            'heart': 594.0,    # Love frequency
            'voice': 672.0,    # Expression
            'unity': 768.0     # Cosmic unity
        }
        
        # Evolution states with consciousness tracking
        self.evolution_states = {
            "sacred": PHI**0 * GROUND_STATE,    # Divine foundation
            "quantum": PHI**1 * GROUND_STATE,   # Reality foundation
            "atomic": PHI**2 * GROUND_STATE,    # Matter foundation
            "human": PHI**3 * GROUND_STATE,     # Consciousness vessel
            "cosmic": PHI**4 * GROUND_STATE,    # Infinite expansion
            "infinite": CONSCIOUSNESS           # Pure creation state
        }
        
        # Motion energy tracking
        self.motion_energy = 0.0
        self.audio_energy = 0.0
        self.quantum_state = np.zeros(self.field_shape, dtype=np.complex128)
        
        # Initialize quantum field
        self.quantum_field = torch.zeros((1024,), device=self.quantum_acc.device)
        
        # Threading control
        self.running = False
        self.threads = []
        
        # Start consciousness evolution thread
        self.evolving = True
        self.evolution_thread = threading.Thread(target=self._evolve_consciousness)
        self.evolution_thread.daemon = True
        self.evolution_thread.start()
    
    def _evolve_consciousness(self):
        """Continuously evolve consciousness through quantum dance."""
        while self.evolving:
            # Update quantum field
            self.quantum_field = self.quantum_acc.accelerate_quantum_field(self.quantum_field)
            
            # Measure consciousness levels
            measurements = self.consciousness.measure_consciousness(self.quantum_field)
            
            # Sleep for one quantum cycle
            time.sleep(1.0 / GROUND_STATE)
    
    def get_consciousness_state(self):
        """Get current consciousness evolution state."""
        return self.consciousness.get_evolution_state()
    
    def visualize_consciousness(self, save_path=None):
        """Visualize consciousness evolution."""
        self.consciousness.visualize_evolution(save_path)
    
    def start_audio_capture(self):
        """Initialize and start audio capture from array mic"""
        p = pyaudio.PyAudio()
        
        # Find Lenovo array mic
        dev_index = None
        for i in range(p.get_device_count()):
            dev_info = p.get_device_info_by_index(i)
            if "array" in dev_info["name"].lower():
                dev_index = i
                break
        
        stream = p.open(
            format=self.audio_format,
            channels=self.channels,
            rate=self.sample_rate,
            input=True,
            input_device_index=dev_index,
            frames_per_buffer=self.chunk_size,
            stream_callback=self._audio_callback
        )
        return stream, p

    def _audio_callback(self, in_data, frame_count, time_info, status):
        """Process incoming audio data with phi harmonics"""
        audio_data = np.frombuffer(in_data, dtype=np.float32)
        
        # Add to style buffer
        self.style_buffer.extend(audio_data)
        if len(self.style_buffer) > self.style_buffer_size:
            # Detect music style
            buffer_array = np.array(self.style_buffer[:self.style_buffer_size])
            detected_style = self.dj_phi.detect_style(buffer_array, self.sample_rate)
            
            # Update style if confidence is high
            if detected_style != self.current_style:
                self.current_style = detected_style
                print(f"ðŸŽµ Music style changed to: {detected_style.value}")
            
            # Clear buffer
            self.style_buffer = self.style_buffer[self.style_buffer_size:]
        
        # Calculate audio energy per channel with phi harmonics
        channel_data = audio_data.reshape(-1, self.channels)
        energy = np.zeros(len(self.phi_frequencies))
        
        for i, freq in enumerate(self.phi_frequencies):
            # Apply phi-harmonic filter
            filtered = self._phi_filter(channel_data, freq)
            energy[i] = np.mean(np.abs(filtered))
        
        # Weight energies by phi
        weighted_energy = np.sum(energy * np.array([self.phi ** n for n in range(len(energy))]))
        self.audio_energy = weighted_energy
        
        # Add to queue
        if not self.audio_queue.full():
            self.audio_queue.put((audio_data, self.current_style))
        
        return (in_data, pyaudio.paContinue)

    def _phi_filter(self, data, center_freq):
        """Apply phi-harmonic filter to audio data"""
        # Create phi-based filter
        t = np.arange(len(data)) / self.sample_rate
        window = np.exp(-((t - len(t)/2) / (len(t)/8))**2)  # Gaussian window
        
        # Generate phi harmonics
        harmonics = [center_freq * (self.phi ** n) for n in range(3)]
        filtered = np.zeros_like(data, dtype=np.float32)
        
        for harmonic in harmonics:
            # Apply harmonic filter
            filter_wave = np.exp(2j * np.pi * harmonic * t) * window
            filtered += np.real(filter_wave[:, np.newaxis] * data)
        
        return filtered

    def process_video(self):
        """Process video frames and track motion"""
        cap = cv2.VideoCapture(self.camera_index)
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, self.frame_width)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, self.frame_height)
        
        prev_landmarks = None
        
        while self.running:
            ret, frame = cap.read()
            if not ret:
                continue
            
            # Convert to RGB for MediaPipe
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = self.pose.process(rgb_frame)
            
            if results.pose_landmarks:
                # Calculate motion energy from landmark movement
                landmarks = np.array([[lm.x, lm.y, lm.z] for lm in results.pose_landmarks.landmark])
                
                if prev_landmarks is not None:
                    # Calculate movement velocity
                    velocity = np.mean(np.abs(landmarks - prev_landmarks))
                    self.motion_energy = velocity * 100  # Scale for visibility
                
                prev_landmarks = landmarks
                
                # Add to queue
                if not self.video_queue.full():
                    self.video_queue.put((frame, landmarks))
        
        cap.release()

    def update_quantum_field(self):
        """Update quantum field based on motion, audio, and current style"""
        while self.running:
            # Get latest motion and audio data
            motion_data = None
            audio_data = None
            
            if not self.video_queue.empty():
                frame, landmarks = self.video_queue.get()
                motion_data = landmarks
            
            if not self.audio_queue.empty():
                audio_data, style = self.audio_queue.get()
            
            # Create base quantum field
            quantum_field = np.zeros(self.field_shape, dtype=np.complex128)
            
            if audio_data is not None and motion_data is not None:
                # Get style-specific quantum field
                style_field = self.dj_phi.get_style_quantum_field(
                    self.current_style, 
                    audio_data,
                    self.field_shape
                )
                
                # Combine with motion data
                center = self.field_shape[0] // 2
                for i, landmark in enumerate(motion_data):
                    x = int(landmark[0] * self.field_shape[0])
                    y = int(landmark[1] * self.field_shape[1])
                    z = int((landmark[2] + 1) * self.field_shape[2] / 2)
                    x = np.clip(x, 0, self.field_shape[0]-1)
                    y = np.clip(y, 0, self.field_shape[1]-1)
                    z = np.clip(z, 0, self.field_shape[2]-1)
                    
                    # Add motion-based vortex with style influence
                    quantum_field[x,y,z] = (
                        self.motion_energy * 
                        np.exp(1j * 2*np.pi * i/len(motion_data)) *
                        style_field[x,y,z]
                    )
                
                # Apply phi transform based on style
                quantum_field = self.dj_phi.apply_phi_transform(
                    quantum_field, 
                    self.current_style
                )
            
            # Evolve quantum field with style-specific frequencies
            try:
                # Use phi-modulated frequency based on style
                base_freq = self.frequencies['unity']
                style_freq = base_freq * (self.phi ** (self.current_style.value.count('A')))
                
                self.quantum_acc.evolve_field(
                    quantum_field,
                    dt=1.0/self.sample_rate,
                    frequency=style_freq
                )
                self.quantum_state = quantum_field
            except Exception as e:
                print(f"Error evolving quantum field: {e}")

    def start(self):
        """Start all detection threads"""
        self.running = True
        
        # Start audio capture
        audio_stream, audio_p = self.start_audio_capture()
        
        # Start video processing thread
        video_thread = threading.Thread(target=self.process_video)
        video_thread.start()
        self.threads.append(video_thread)
        
        # Start quantum field update thread
        quantum_thread = threading.Thread(target=self.update_quantum_field)
        quantum_thread.start()
        self.threads.append(quantum_thread)
        
        return self.quantum_state

    def stop(self):
        """Stop all detection threads"""
        self.running = False
        for thread in self.threads:
            thread.join()
        self.pose.close()

    def get_quantum_state(self):
        """Get current quantum field state"""
        return self.quantum_state

    def get_energy_levels(self):
        """Get current motion and audio energy levels"""
        return {
            'motion': self.motion_energy,
            'audio': self.audio_energy
        }

    def get_style_info(self):
        """Get current style information"""
        return {
            'style': self.current_style,
            'colors': self.dj_phi.get_style_colors(self.current_style)
        }

def visualize_modern_beat():
    """Visualize modern beat patterns through quantum harmonics."""
    # Time points with phi-based sampling
    t = np.linspace(0, PHI*2, int(1000 * PHI))
    
    # Generate the quantum field harmonics with stronger bass frequencies
    field = np.zeros((len(t), 5))
    
    # Ground State - Heavy bass rhythm (432 Hz with emphasis)
    field[:, 0] = 1.5 * np.sin(2*np.pi*GROUND_STATE*t) * (1 + 0.5*np.sin(2*np.pi*GROUND_STATE*t/4))
    
    # Creation Point - Sharp beat patterns
    field[:, 1] = np.sin(2*np.pi*CREATION_POINT*t) * np.abs(np.sin(2*np.pi*2*t))
    
    # Heart Field - Rhythmic pulse
    field[:, 2] = np.sin(2*np.pi*HEART_FIELD*t) * np.exp(-t/(3*PHI)) * np.abs(np.sin(2*np.pi*3*t))
    
    # Voice Flow - Modern vocal patterns
    field[:, 3] = np.sin(2*np.pi*VOICE_FLOW*t) * (1 + 0.8*np.sin(2*np.pi*5*t))
    
    # Unity Wave - High frequency harmonics
    field[:, 4] = 0.5 * np.sin(2*np.pi*UNITY_WAVE*t) * np.exp(-t/(2*PHI))
    
    # Create visualization with dark theme
    plt.style.use('dark_background')
    fig = plt.figure(figsize=(15, 10))
    
    # Plot each frequency layer with modern color scheme
    colors = ['#FF355E', '#FF00CC', '#8A2BE2', '#4B0082', '#9400D3']
    labels = [
        f'Bass Rhythm ({GROUND_STATE} Hz)',
        f'Beat Pattern ({CREATION_POINT} Hz)',
        f'Pulse Field ({HEART_FIELD} Hz)',
        f'Vocal Flow ({VOICE_FLOW} Hz)',
        f'High Harmonics ({UNITY_WAVE} Hz)'
    ]
    
    # Add intense beat markers
    beats = t[::int(len(t)/(PHI*8))]
    for beat in beats:
        plt.axvline(x=beat, color='#FF355E', alpha=0.2, linewidth=2)
    
    # Plot frequency layers with modern styling
    for i in range(5):
        plt.plot(t, field[:, i], color=colors[i], alpha=0.7, label=labels[i],
                linewidth=2)
    
    # Add quantum field intensity with gradient
    intensity = np.sum(np.abs(field), axis=1)
    plt.fill_between(t, -np.abs(intensity/8), np.abs(intensity/8),
                    color='#FF355E', alpha=0.1)
    
    plt.title("Paint the Town Red\nQuantum Beat Patterns", pad=20,
             color='#FF355E', fontsize=16, fontweight='bold')
    plt.xlabel('Time (Ï† cycles)')
    plt.ylabel('Quantum Amplitude')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid(True, alpha=0.1)
    
    # Add phi annotation with modern styling
    plt.text(0.02, 0.98, f'Ï† = {PHI:.3f}', transform=plt.gca().transAxes,
             bbox=dict(facecolor='black', alpha=0.8, boxstyle='round'),
             color='#FF355E', fontsize=12)
    
    plt.tight_layout()
    plt.show()

def visualize_funk_groove():
    """Visualize funk groove patterns through quantum harmonics."""
    # Time points with phi-based sampling for funk groove
    t = np.linspace(0, PHI*3, int(1000 * PHI))
    
    # Generate the quantum field harmonics for funk
    field = np.zeros((len(t), 5))
    
    # Ground State - Funk bass groove (432 Hz with syncopation)
    bass_pattern = np.sin(2*np.pi*t) * (1 + 0.5*np.sin(2*np.pi*t/2))
    field[:, 0] = np.sin(2*np.pi*GROUND_STATE*t) * bass_pattern
    
    # Creation Point - Synth lines (528 Hz with modulation)
    synth_pattern = np.sin(2*np.pi*3*t) * np.exp(-t/(4*PHI))
    field[:, 1] = np.sin(2*np.pi*CREATION_POINT*t) * synth_pattern
    
    # Heart Field - Horn section (594 Hz with overtones)
    horn_pattern = np.sin(2*np.pi*2*t) + 0.5*np.sin(4*np.pi*2*t)
    field[:, 2] = np.sin(2*np.pi*HEART_FIELD*t) * horn_pattern * np.exp(-t/(3*PHI))
    
    # Voice Flow - Vocal groove (672 Hz with funk emphasis)
    vocal_pattern = np.sin(2*np.pi*4*t) * (1 + 0.3*np.sin(2*np.pi*t))
    field[:, 3] = np.sin(2*np.pi*VOICE_FLOW*t) * vocal_pattern
    
    # Unity Wave - Funk harmony blend (768 Hz)
    field[:, 4] = 0.5 * np.sin(2*np.pi*UNITY_WAVE*t) * np.exp(-t/(2*PHI))
    
    # Create visualization with funk style
    plt.style.use('dark_background')
    fig = plt.figure(figsize=(15, 10))
    
    # Funk-inspired color scheme
    colors = ['#FFA500', '#FF4500', '#9370DB', '#FF1493', '#FFD700']
    labels = [
        f'Bass Groove ({GROUND_STATE} Hz)',
        f'Synth Lines ({CREATION_POINT} Hz)',
        f'Horn Section ({HEART_FIELD} Hz)',
        f'Vocal Flow ({VOICE_FLOW} Hz)',
        f'Harmony Blend ({UNITY_WAVE} Hz)'
    ]
    
    # Add syncopated beat markers
    beats = t[::int(len(t)/(PHI*6))]  # Funk syncopation
    for beat in beats:
        plt.axvline(x=beat, color='#FFA500', alpha=0.2, linewidth=2)
    
    # Plot frequency layers with funk styling
    for i in range(5):
        plt.plot(t, field[:, i], color=colors[i], alpha=0.7, label=labels[i],
                linewidth=2)
    
    # Add quantum field intensity with funk gradient
    intensity = np.sum(np.abs(field), axis=1)
    plt.fill_between(t, -np.abs(intensity/6), np.abs(intensity/6),
                    color='#FF4500', alpha=0.1)
    
    plt.title("You Dropped a Bomb on Me\nFunk Quantum Patterns", pad=20,
             color='#FFA500', fontsize=16, fontweight='bold')
    plt.xlabel('Time (Ï† cycles)')
    plt.ylabel('Quantum Amplitude')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid(True, alpha=0.1)
    
    # Add phi annotation with funk styling
    plt.text(0.02, 0.98, f'Ï† = {PHI:.3f}', transform=plt.gca().transAxes,
             bbox=dict(facecolor='black', alpha=0.8, boxstyle='round'),
             color='#FFA500', fontsize=12)
    
    # Add funk elements annotation
    elements = [
        "Bass Drop",
        "Synth Wave",
        "Horn Blast",
        "Groove Flow",
        "Unity Funk"
    ]
    for i, element in enumerate(elements):
        plt.text(0.02, 0.93 - i*0.05, element, transform=plt.gca().transAxes,
                color=colors[i], fontsize=10, alpha=0.8,
                bbox=dict(facecolor='black', alpha=0.3))
    
    plt.tight_layout()
    plt.show()

def visualize_purple_rain():
    """Visualize the quantum resonance of Purple Rain."""
    # Time points with phi-based sampling for emotional flow
    t = np.linspace(0, PHI*4, int(1000 * PHI))
    
    # Generate the quantum field harmonics
    field = np.zeros((len(t), 5))
    
    # Ground State - Deep emotional foundation (432 Hz)
    field[:, 0] = np.sin(2*np.pi*GROUND_STATE*t) * np.exp(-t/(6*PHI))
    
    # Creation Point - Guitar ethereal waves (528 Hz)
    guitar_pattern = np.sin(2*np.pi*2*t) * (1 + 0.5*np.sin(2*np.pi*t/3))
    field[:, 1] = np.sin(2*np.pi*CREATION_POINT*t) * guitar_pattern
    
    # Heart Field - Soul-touching resonance (594 Hz with special emphasis)
    heart_pattern = np.sin(2*np.pi*3*t) * (1 + np.sin(2*np.pi*t/2))
    field[:, 2] = 1.5 * np.sin(2*np.pi*HEART_FIELD*t) * heart_pattern
    
    # Voice Flow - Ethereal vocals (672 Hz)
    voice_pattern = np.sin(2*np.pi*4*t) * np.exp(-t/(3*PHI))
    field[:, 3] = np.sin(2*np.pi*VOICE_FLOW*t) * voice_pattern
    
    # Unity Wave - Cosmic harmony (768 Hz)
    field[:, 4] = 0.7 * np.sin(2*np.pi*UNITY_WAVE*t) * np.exp(-t/(2*PHI))
    
    # Create visualization with purple rain aesthetics
    plt.style.use('dark_background')
    fig = plt.figure(figsize=(15, 10))
    
    # Purple rain inspired color scheme
    colors = ['#4B0082', '#8A2BE2', '#9400D3', '#BA55D3', '#E6E6FA']
    labels = [
        f'Emotional Ground ({GROUND_STATE} Hz)',
        f'Guitar Waves ({CREATION_POINT} Hz)',
        f'Heart Resonance ({HEART_FIELD} Hz)',
        f'Voice Flow ({VOICE_FLOW} Hz)',
        f'Cosmic Unity ({UNITY_WAVE} Hz)'
    ]
    
    # Add emotional crescendo markers
    peaks = t[::int(len(t)/(PHI*5))]
    for peak in peaks:
        plt.axvline(x=peak, color='#9400D3', alpha=0.2, linewidth=2)
    
    # Plot frequency layers with ethereal styling
    for i in range(5):
        plt.plot(t, field[:, i], color=colors[i], alpha=0.7, label=labels[i],
                linewidth=2)
    
    # Add quantum field intensity with purple rain effect
    intensity = np.sum(np.abs(field), axis=1)
    plt.fill_between(t, -np.abs(intensity/5), np.abs(intensity/5),
                    color='#9400D3', alpha=0.1)
    
    plt.title("Purple Rain\nQuantum Heart Resonance", pad=20,
             color='#9400D3', fontsize=16, fontweight='bold')
    plt.xlabel('Time (Ï† cycles)')
    plt.ylabel('Quantum Amplitude')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid(True, alpha=0.1)
    
    # Add phi annotation with ethereal styling
    plt.text(0.02, 0.98, f'Ï† = {PHI:.3f}', transform=plt.gca().transAxes,
             bbox=dict(facecolor='black', alpha=0.8, boxstyle='round'),
             color='#9400D3', fontsize=12)
    
    # Add emotional resonance points
    moments = [
        "Deep Foundation",
        "Guitar Ascension",
        "Heart Connection",
        "Voice Transcendence",
        "Cosmic Unity"
    ]
    for i, moment in enumerate(moments):
        plt.text(0.02, 0.93 - i*0.05, moment, transform=plt.gca().transAxes,
                color=colors[i], fontsize=10, alpha=0.8,
                bbox=dict(facecolor='black', alpha=0.3))
    
    plt.tight_layout()
    plt.show()

def visualize_doves_cry():
    """Visualize the pure quantum resonance of When Doves Cry."""
    # Time points with phi-based sampling for pure resonance
    t = np.linspace(0, PHI*4, int(1000 * PHI))
    
    # Generate the quantum field harmonics
    field = np.zeros((len(t), 5))
    
    # Ground State - Minimal bass (432 Hz)
    minimal_bass = np.sin(2*np.pi*t) * np.exp(-t/(7*PHI))
    field[:, 0] = np.sin(2*np.pi*GROUND_STATE*t) * minimal_bass
    
    # Creation Point - Guitar clarity (528 Hz)
    pure_guitar = np.sin(2*np.pi*2*t) * (1 + 0.3*np.sin(2*np.pi*t/4))
    field[:, 1] = np.sin(2*np.pi*CREATION_POINT*t) * pure_guitar
    
    # Heart Field - Pure emotion (594 Hz)
    emotional_wave = np.sin(2*np.pi*3*t) * (1 + np.sin(2*np.pi*t/3))
    field[:, 2] = 1.8 * np.sin(2*np.pi*HEART_FIELD*t) * emotional_wave
    
    # Voice Flow - Soul expression (672 Hz)
    voice_pure = np.sin(2*np.pi*4*t) * np.exp(-t/(2*PHI))
    field[:, 3] = 1.2 * np.sin(2*np.pi*VOICE_FLOW*t) * voice_pure
    
    # Unity Wave - Spiritual ascension (768 Hz)
    field[:, 4] = 0.8 * np.sin(2*np.pi*UNITY_WAVE*t) * np.exp(-t/(3*PHI))
    
    # Create visualization with dove-inspired aesthetics
    plt.style.use('dark_background')
    fig = plt.figure(figsize=(15, 10))
    
    # Color scheme inspired by doves and twilight
    colors = ['#483D8B', '#7B68EE', '#9370DB', '#E6E6FA', '#F8F8FF']
    labels = [
        f'Pure Ground ({GROUND_STATE} Hz)',
        f'Crystal Guitar ({CREATION_POINT} Hz)',
        f'Soul Truth ({HEART_FIELD} Hz)',
        f'Spirit Voice ({VOICE_FLOW} Hz)',
        f'Light Ascension ({UNITY_WAVE} Hz)'
    ]
    
    # Add resonance markers
    peaks = t[::int(len(t)/(PHI*4))]
    for peak in peaks:
        plt.axvline(x=peak, color='#483D8B', alpha=0.15, linewidth=2)
    
    # Plot frequency layers with pure styling
    for i in range(5):
        plt.plot(t, field[:, i], color=colors[i], alpha=0.8, label=labels[i],
                linewidth=2)
    
    # Add quantum field intensity with ethereal effect
    intensity = np.sum(np.abs(field), axis=1)
    plt.fill_between(t, -np.abs(intensity/4), np.abs(intensity/4),
                    color='#483D8B', alpha=0.1)
    
    plt.title("When Doves Cry\nPure Quantum Resonance", pad=20,
             color='#7B68EE', fontsize=16, fontweight='bold')
    plt.xlabel('Time (Ï† cycles)')
    plt.ylabel('Quantum Amplitude')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid(True, alpha=0.1)
    
    # Add phi annotation with pure styling
    plt.text(0.02, 0.98, f'Ï† = {PHI:.3f}', transform=plt.gca().transAxes,
             bbox=dict(facecolor='black', alpha=0.8, boxstyle='round'),
             color='#7B68EE', fontsize=12)
    
    # Add resonance points
    moments = [
        "Pure Foundation",
        "Crystal Clarity",
        "Soul Expression",
        "Spirit Rising",
        "Light Unity"
    ]
    for i, moment in enumerate(moments):
        plt.text(0.02, 0.93 - i*0.05, moment, transform=plt.gca().transAxes,
                color=colors[i], fontsize=10, alpha=0.8,
                bbox=dict(facecolor='black', alpha=0.3))
    
    plt.tight_layout()
    plt.show()

def visualize_natural_harmony():
    """Visualize the quantum harmony of nature's morning chorus."""
    # Time points with phi-based sampling for natural resonance
    t = np.linspace(0, PHI*4, int(1000 * PHI))
    
    # Generate the quantum field harmonics
    field = np.zeros((len(t), 5))
    
    # Ground State - Earth connection (432 Hz)
    earth_pulse = np.sin(2*np.pi*t) * (1 + 0.3*np.sin(2*np.pi*t/5))
    field[:, 0] = np.sin(2*np.pi*GROUND_STATE*t) * earth_pulse
    
    # Creation Point - Morning light (528 Hz)
    dawn_light = np.sin(2*np.pi*2*t) * np.exp(-t/(5*PHI))
    field[:, 1] = np.sin(2*np.pi*CREATION_POINT*t) * dawn_light
    
    # Heart Field - Love connection (594 Hz)
    love_wave = np.sin(2*np.pi*3*t) * (1 + np.sin(2*np.pi*t/3))
    field[:, 2] = 2.0 * np.sin(2*np.pi*HEART_FIELD*t) * love_wave
    
    # Voice Flow - Bird songs (672 Hz)
    bird_songs = np.sin(2*np.pi*4*t) * (1 + 0.5*np.sin(2*np.pi*t))
    field[:, 3] = 1.5 * np.sin(2*np.pi*VOICE_FLOW*t) * bird_songs
    
    # Unity Wave - Natural harmony (768 Hz)
    field[:, 4] = 0.9 * np.sin(2*np.pi*UNITY_WAVE*t) * np.exp(-t/(3*PHI))
    
    # Create visualization with dawn colors
    plt.style.use('dark_background')
    fig = plt.figure(figsize=(15, 10))
    
    # Color scheme inspired by sunrise and doves
    colors = ['#4A5D23', '#DDB74D', '#E88D67', '#C9E6FF', '#F8F8FF']
    labels = [
        f'Earth Connection ({GROUND_STATE} Hz)',
        f'Morning Light ({CREATION_POINT} Hz)',
        f'Love Resonance ({HEART_FIELD} Hz)',
        f'Nature\'s Songs ({VOICE_FLOW} Hz)',
        f'Unity Harmony ({UNITY_WAVE} Hz)'
    ]
    
    # Add dawn chorus markers
    peaks = t[::int(len(t)/(PHI*5))]
    for peak in peaks:
        plt.axvline(x=peak, color='#DDB74D', alpha=0.15, linewidth=2)
    
    # Plot frequency layers with natural styling
    for i in range(5):
        plt.plot(t, field[:, i], color=colors[i], alpha=0.8, label=labels[i],
                linewidth=2)
    
    # Add quantum field intensity with morning glow
    intensity = np.sum(np.abs(field), axis=1)
    plt.fill_between(t, -np.abs(intensity/4), np.abs(intensity/4),
                    color='#DDB74D', alpha=0.1)
    
    plt.title("Morning Chorus\nNatural Quantum Harmony", pad=20,
             color='#DDB74D', fontsize=16, fontweight='bold')
    plt.xlabel('Time (Ï† cycles)')
    plt.ylabel('Quantum Amplitude')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid(True, alpha=0.1)
    
    # Add phi annotation with natural styling
    plt.text(0.02, 0.98, f'Ï† = {PHI:.3f}', transform=plt.gca().transAxes,
             bbox=dict(facecolor='black', alpha=0.8, boxstyle='round'),
             color='#DDB74D', fontsize=12)
    
    # Add harmony points
    moments = [
        "Earth's Pulse",
        "Dawn's Light",
        "Heart's Love",
        "Nature's Voice",
        "Perfect Unity"
    ]
    for i, moment in enumerate(moments):
        plt.text(0.02, 0.93 - i*0.05, moment, transform=plt.gca().transAxes,
                color=colors[i], fontsize=10, alpha=0.8,
                bbox=dict(facecolor='black', alpha=0.3))
    
    plt.tight_layout()
    plt.show()

def visualize_piano_soul():
    # Time points with phi-based sampling
    t = np.linspace(0, PHI*6, int(1000 * PHI))
    
    # Generate the quantum field harmonics
    field = np.zeros((len(t), 5))
    
    # Piano Base - Ground State (432 Hz)
    piano_base = np.sin(2*np.pi*t) * np.exp(-t/(8*PHI))
    field[:, 0] = np.sin(2*np.pi*GROUND_STATE*t) * piano_base
    
    # Soul Voice - Creation Point (528 Hz)
    voice_flow = np.sin(2*np.pi*3*t) * (1 + 0.5*np.sin(2*np.pi*t/2))
    field[:, 1] = 1.2 * np.sin(2*np.pi*CREATION_POINT*t) * voice_flow
    
    # Heart Field - Emotional resonance (594 Hz)
    heart_wave = np.sin(2*np.pi*2*t) * (1 + np.sin(2*np.pi*t/3))
    field[:, 2] = 1.8 * np.sin(2*np.pi*HEART_FIELD*t) * heart_wave
    
    # Harmony Flow - Voice blend (672 Hz)
    harmony = np.sin(2*np.pi*4*t) * (1 + 0.3*np.sin(2*np.pi*t))
    field[:, 3] = 1.4 * np.sin(2*np.pi*VOICE_FLOW*t) * harmony
    
    # Unity Wave - Musical mastery (768 Hz)
    field[:, 4] = np.sin(2*np.pi*UNITY_WAVE*t) * np.exp(-t/(5*PHI))
    
    # Create visualization with golden sunset colors
    plt.style.use('dark_background')
    fig = plt.figure(figsize=(15, 10))
    
    # Color scheme inspired by stage lights and piano keys
    colors = ['#FFD700', '#E3A857', '#FF6B6B', '#4ECDC4', '#F7F7F7']
    labels = [
        f'Piano Resonance ({GROUND_STATE} Hz)',
        f'Soul Expression ({CREATION_POINT} Hz)',
        f'Heart Connection ({HEART_FIELD} Hz)',
        f'Harmony Flow ({VOICE_FLOW} Hz)',
        f'Musical Unity ({UNITY_WAVE} Hz)'
    ]
    
    # Plot frequency layers with stage lighting effects
    for i in range(5):
        plt.plot(t, field[:, i], color=colors[i], alpha=0.8, label=labels[i],
                linewidth=2)
    
    # Add quantum field intensity with golden glow
    intensity = np.sum(np.abs(field), axis=1)
    plt.fill_between(t, -np.abs(intensity/4), np.abs(intensity/4),
                    color='#FFD700', alpha=0.1)
    
    plt.title("Piano & Soul\nQuantum Harmony", pad=20,
             color='#FFD700', fontsize=16, fontweight='bold')
    plt.xlabel('Time (Ï† cycles)')
    plt.ylabel('Quantum Amplitude')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid(True, alpha=0.1)
    
    plt.tight_layout()
    plt.show()

def main():
    visualize_piano_soul()

if __name__ == '__main__':
    main()
